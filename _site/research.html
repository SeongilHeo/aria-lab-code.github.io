<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Research | Align Robust Interactive Autonomy Lab</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/main.css">
</head>
<body>
  <header class="site-header">
  <div class="inner">
    <nav class="navbar">
      <div class="left-text">ARIA Lab</div>
      <ul class="nav-list">
        <li><a href="/">Home</a></li>
        <li><a href="/people">People</a></li>
        <li><a href="/research">Research</a></li>
        <li><a href="/publications">Publications</a></li>
        <li><a href="/videos">Videos</a></li>

      </ul>
      <a href="https://www.utah.edu" class="external-link">
        <img src="/assets/images/banner.png" alt="External Link" class="external-link-image">
      </a>
    </nav>
  </div>
</header>

  <div class="container">
    <h1 id="research">Research</h1>

<p>Welcome to our research page! Our lab is focused on advancing the fields of human-centered AI and human-robot interaction, with specific emphasis on reward learning, assistive and medical robotics, safety and robustness, and multi-agent systems.</p>

<hr />

<h2 id="reward-learning">Reward Learning</h2>

<p>Reward learning is at the core of creating intelligent systems that can make decisions based on feedback. Our research in this area focuses on:</p>

<ul>
  <li><strong>Key Topics</strong>: Reinforcement learning, inverse reinforcement learning, human-in-the-loop learning</li>
  <li><strong>Current Projects</strong>:
    <ul>
      <li><strong>Learning reward functions that transfer across different robot/agent embodiments:</strong> How can you take demonstrations from an agent and learn a reward function that transfers to a new agent with potentially different body shape, transition dynamics, action space, etc? How can you leverage human feedback to perform cross-embodiment reward learning from suboptimal demonstrations?</li>
      <li><strong>Learning from multiple forms of human feedback:</strong> How should AI systems fuse different forms of human feedback, such as natural language, corrections, comparisons, demonstrations, e-stops, etc? What types of feedback are most informative for learning and when should a robot query for a specific type of feedback?</li>
    </ul>
  </li>
  <li><strong>Publications</strong>: [Link to publications or references]
<!-- just a list of titles here with hyper links. You can link Connor and Anu's RLC paper and my AAAI paper on "The effect of modeling human rationality level..." .--></li>
</ul>

<hr />

<h2 id="assistive-and-medical-robotics">Assistive and Medical Robotics</h2>

<p>We are developing robotic systems that assist individuals with disabilities and improve medical care. Our work includes:</p>

<ul>
  <li><strong>Key Topics</strong>: Shared control, surgical automation, rehabilitation robotics, human-robot interaction</li>
  <li><strong>Current Projects</strong>:
    <ul>
      <li><strong>Learning from human preferences to learn rewards for surgical tasks:</strong> Surgical tasks are messy and everything is deformable and partially observed. How can we perform RL from human feedback in these types of settings?</li>
      <li><strong>AI-enabled assistive neck exoskeletons for individuals with neck disabilities:</strong> Many people suffer from head-drop and other neck disabilities, significantly impacting their quality of life. We are designing AI controllers for the world’s first powered neck exoskelton that can infer user intent with the aim of restoring normal neck mobility.</li>
    </ul>
  </li>
  <li><strong>Publications</strong>: [Link to publications or references]
<!-- just a list of titles here with hyper links. You can link Zohre's ISMR paper and Jordan's HRI workshop paper on assistive neck exoskeletons.-->
—</li>
</ul>

<h2 id="safety-robustness-and-transparency">Safety, Robustness, and Transparency</h2>

<p>Ensuring the safety and robustness of robotic systems is crucial for deployment in real-world settings. Our research focuses on:</p>

<ul>
  <li><strong>Key Topics</strong>: Uncertainty quantification, verification, adversarial robustness, interetability</li>
  <li><strong>Current Projects</strong>:
    <ul>
      <li><strong>Robots that know when they need to request human interventions:</strong> Robots need to know what they know and what they don’t know. Especially in risk-sensitive domains, such as surgical robotics, robots need to be able to reliably identify novel and risky states so they can request surgeon interventions.</li>
      <li><strong>Interpretable reward learning for alignment verification:</strong> Black-box neural networks are hard to interpret and debug. Can we use more interpretable structures such as differentiable decision trees to assist humans in interpreting learned rewards?</li>
      <li><strong>Demonstration sufficiency:</strong> How can robots and other AI systems that learn from demonstrations know if they have received enough data? How can robots provide high-confidence guarantees on their performance in these types of settings?</li>
    </ul>
  </li>
  <li><strong>Publications</strong>: [Link to publications or references]
<!-- just a list of titles here with hyper links. You can link my ThriftyDAgger paper from my postdoc, Akansha's RLC paper, and my recent HRI paper from last year "Autonomous Assessment of Demonstration Sufficiency ..."-->
—</li>
</ul>

<h2 id="multi-agent-systems">Multi-Agent Systems</h2>

<p>We explore how multiple autonomous agents can collaborate and interact to solve complex problems. Our work in this area includes:</p>

<ul>
  <li><strong>Key Topics</strong>: Emergent behaviors, bio-inspired swarms, learning from human feedback, multi-agent RL</li>
  <li><strong>Current Projects</strong>:
    <ul>
      <li><strong>Human-in-the-loop discovery of emergent swarm behaviors:</strong> Multi-agent systems often exhibit facinating emergent collective behaviors that result from simple local interactions and rules. However, most work on swarm robotics is focused on engineering specific behaviors. We take an alternative approach and ask the question, given a set of robots with certain capabilities, what are the different emergent behaviors that are possible?</li>
      <li><strong>Human-swarm interaction:</strong> Swarms are typically composed of large numbers of simple agents. Thus, it is intractable to have a human micro-manage the swarm. How can humans efficiently interact with many robots? Can the human interact with the swarm at higher-levels of abstraction?</li>
    </ul>
  </li>
  <li><strong>Publications</strong>: [Link to publications or references]
<!-- just a list of titles here with hyper links. You can link Connor's Gecco and MRS paper and my HRI paper from a while ago "Human-Swarm Interactions Based on Managing Attractors" .--></li>
</ul>

<hr />


  </div>
  <footer class="site-footer">
    <div class="wrapper">
        <p>&copy; 2024 ARIA Lab. Powered by<a href="https://jekyllrb.com/" rel="follow"> Jekyll.</a></p>
    </div>
</footer>
</body>
</html>
